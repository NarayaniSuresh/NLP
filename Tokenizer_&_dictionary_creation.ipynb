{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVLOOd19epTulgiZI9v83M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NarayaniSuresh/NLP/blob/main/Tokenizer_%26_dictionary_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml-oaPm9X501",
        "outputId": "d3313fb3-7a2c-4b89-cb92-39700292732b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dog', 'is', 'not', 'lazy', 'after', 'all', ',', 'he', \"'s\", 'just', 'tired', '.']\n",
            "Vocabulary:  {'tired', 'he', 'over', '.', \"'s\", ',', 'all', 'the', 'jumps', 'is', 'after', 'just', 'brown', 'lazy', 'fox', 'dog', 'quick', 'The', 'not'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample paragraph\n",
        "text = \"The quick brown fox jumps over the lazy dog. The dog is not lazy after all, he's just tired.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens: \", tokens)\n",
        "\n",
        "# Create a vocabulary\n",
        "vocab = set(tokens)\n",
        "print(\"Vocabulary: \", vocab)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I don't like bananas\", \"They're always late\", \"This was co-created by my friend\", \"This document was signed '12-02-24'\"]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokens_list = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(\"Tokenized sentences: \", tokens_list)\n",
        "\n",
        "# Create a vocabulary for each sentence\n",
        "vocab_list = [set(tokens) for tokens in tokens_list]\n",
        "print(\"Vocabularies: \", vocab_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHnGx7wkavpq",
        "outputId": "370b9ada-0b04-4745-dcfa-878d08ec0fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences:  [['I', 'do', \"n't\", 'like', 'bananas'], ['They', \"'re\", 'always', 'late'], ['This', 'was', 'co-created', 'by', 'my', 'friend'], ['This', 'document', 'was', 'signed', \"'12-02-24\", \"'\"]]\n",
            "Vocabularies:  [{\"n't\", 'I', 'do', 'like', 'bananas'}, {'They', 'late', 'always', \"'re\"}, {'friend', 'This', 'by', 'my', 'was', 'co-created'}, {'document', 'signed', 'This', 'was', \"'12-02-24\", \"'\"}]\n"
          ]
        }
      ]
    }
  ]
}